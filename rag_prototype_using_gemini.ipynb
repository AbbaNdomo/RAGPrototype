{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG SYSTEM FOR FINANCIAL DOCUMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PIP Installations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes will need to be imported from their respsective packages: \n",
    "\n",
    "    - google-genai\n",
    "    - azure-search-docments\n",
    "    - langchain-google-genai\n",
    "    - langchain-chroma\n",
    "\n",
    "Note that either one of chroma or azure-search will be used as a vector data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%capture\n",
    "#%pip install langchain-core langchain-community langchain-google-genai\n",
    "#%pip install azure-ai-documentintelligence azure-search-documents\n",
    "#%pip install -U langchain-google-genai\n",
    "#%pip install -U langchain-chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up the Environment to use Google's GenAI llm products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyB9hPjhpqM6THjy_qn8Ne214BLL1MZpobQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing Google's Chat Generative AI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Google Gemini Langchain interation\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=False)\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "#ai_msg = llm.invoke(messages)\n",
    "#ai_msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading  & Chunking documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining a method used to loop through a directory and load all documents inside it.**\n",
    "\n",
    "The loading is carried out by an instance of AzureAIDocumentIntelligenceLoader for the following purposes:\n",
    "\n",
    "    - Ablility to analyze tables, images, and text, extracting all information from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_docs(direc_path, doc_intelligence_endpoint, doc_intelligence_key):\\n    docs = []\\n    for file in os.listdir(direc_path):\\n        if file.endswith(\".pdf\"):\\n            file_path = direc_path + \"/\"+ file\\n            loader = AzureAIDocumentIntelligenceLoader(file_path=file_path, api_endpoint= doc_intelligence_endpoint, api_key=doc_intelligence_key, api_model=\"prebuilt-layout\", mode=\"markdown\", analysis_features= [\"ocrHighResolution\"])\\n            docs += loader.load()\\n    return docs\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "\"\"\"\n",
    "def load_docs(direc_path, doc_intelligence_endpoint, doc_intelligence_key):\n",
    "    docs = []\n",
    "    for file in os.listdir(direc_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = direc_path + \"/\"+ file\n",
    "            loader = AzureAIDocumentIntelligenceLoader(file_path=file_path, api_endpoint= doc_intelligence_endpoint, api_key=doc_intelligence_key, api_model=\"prebuilt-layout\", mode=\"markdown\", analysis_features= [\"ocrHighResolution\"])\n",
    "            docs += loader.load()\n",
    "    return docs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def load_docs(direc_path, LLAMA_CLOUD_API_KEY):\n",
    "    os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_CLOUD_API_KEY\n",
    "    doc_list = []\n",
    "    parsed_content = {}\n",
    "    parsing_10k_instructions = '''The documents attached are various company's financial Form 10-Ks. They explore financial data, providing insight into the respective company's financial status. Answer questions using the information in these financial documents and be precise.'''\n",
    "    def get_text_file_name(file_path):\n",
    "        return file_path.split('/')[-1].split('.')[0]\n",
    "    for file in os.listdir(direc_path):\n",
    "        file_path = direc_path + \"/\"+ file\n",
    "        document = LlamaParse(\n",
    "                            result_type=\"markdown\",\n",
    "                            parsing_instructions=parsing_10k_instructions,\n",
    "                            page_separator=\"\\n=================\\n\",\n",
    "                            skip_diagonal_text=True,\n",
    "                        ).load_data(file_path) # file is file directory string\n",
    "        print(type(document))\n",
    "        doc_list += [Document(page_content=document, id=file)]\n",
    "        #parsed_content[file] = documents\n",
    "\n",
    "    return doc_list\n",
    "    #return parsed_content\n",
    "    #print(parsed_content.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a AzureAIDocumentIntelligence resource noting down it's endpoint and api_key.\n",
    "\n",
    "*OcrHighResolution* is a Read Optical Character Recognition (OCR) model. It runs at a higher resolution than Azure AI Vision Read and extracts print and handwritten text from PDF documents and scanned images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while parsing the file 'C:/Users/abbandomo/OneDrive - KPMG/Desktop/RAG-IN-ABBANDOMO/Shrinked/Apple - Financial Statement Notes.pdf': \n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\npage_content\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m direc_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/abbandomo/OneDrive - KPMG/Desktop/RAG-IN-ABBANDOMO/Shrinked\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#docs = load_docs(direc_path, endpoint, key)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m docs \u001b[38;5;241m=\u001b[39m load_docs(direc_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllx-Ideo4Vk8OrczBqU8CMwe4o5JAZtnnmw8DQZtcHUG8SxrHWb6\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 20\u001b[0m, in \u001b[0;36mload_docs\u001b[1;34m(direc_path, LLAMA_CLOUD_API_KEY)\u001b[0m\n\u001b[0;32m     13\u001b[0m     document \u001b[38;5;241m=\u001b[39m LlamaParse(\n\u001b[0;32m     14\u001b[0m                         result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m                         parsing_instructions\u001b[38;5;241m=\u001b[39mparsing_10k_instructions,\n\u001b[0;32m     16\u001b[0m                         page_separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m                         skip_diagonal_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m                     )\u001b[38;5;241m.\u001b[39mload_data(file_path) \u001b[38;5;66;03m# file is file directory string\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(document))\n\u001b[1;32m---> 20\u001b[0m     doc_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mdocument, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mfile)]\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#parsed_content[file] = documents\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc_list\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\documents\\base.py:270\u001b[0m, in \u001b[0;36mDocument.__init__\u001b[1;34m(self, page_content, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pass page_content in as positional or named arg.\"\"\"\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# my-py is complaining that page_content is not defined on the base class.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Here, we're relying on pydantic base class to handle the validation.\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(page_content\u001b[38;5;241m=\u001b[39mpage_content, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Document\npage_content\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "# Loading sample document (invoice)\n",
    "#url_path = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/rest-api/layout.png\"\n",
    "\n",
    "#endpoint = \"https://docintelone.cognitiveservices.azure.com/\"\n",
    "#key = \"64c61ce74d924ced974b3ee968e50fbe\"\n",
    "\n",
    "# Only if being run on a notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "analysis_features = [\"ocrHighResolution\"]\n",
    "\n",
    "direc_path = \"C:/Users/abbandomo/OneDrive - KPMG/Desktop/RAG-IN-ABBANDOMO/Shrinked\"\n",
    "#docs = load_docs(direc_path, endpoint, key)\n",
    "docs = load_docs(direc_path, \"llx-Ideo4Vk8OrczBqU8CMwe4o5JAZtnnmw8DQZtcHUG8SxrHWb6\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting**\n",
    "\n",
    "The RecursiveCharacterTextSplitter is used because of its simplicity. It breaks down text into smaller *\"chunks\"* depending on your choice of *\"chunk_size\"* and *\"overlap\"*.\n",
    "\n",
    "For a query based system, it is important to minimize the chunk_size to allow for effective retrieval, however, it is also important to have them as large as possible to minimize storage utilisation. \n",
    "\n",
    "*Note that other splitting techniques exist such as Semantic text splitting.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunking using Langchain splitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor split in splits: \\n    print(split.page_content)\\n    print(\"----------------------------------------------------------------------------------\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for split in splits: \n",
    "    print(split.page_content)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embedding Stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up embeddings\n",
    "\n",
    "model = \"models/embedding-001\"\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VECTOR STORE USING AZURE_SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_community.vectorstores.azuresearch import AzureSearch\\n\\nvector_store_address: str = \"https://ragsearchone.search.windows.net\"\\nvector_store_password: str = \"oZfKSbPgirnz2DlPhtHA8KQjcJ8UaRKnpANa7UI16EAzSeC0NfRj\"\\n\\nindex_name: str = \"langchain-rag-prototype\"\\nvectorstore_azure = AzureSearch(\\n    azure_search_endpoint=vector_store_address,\\n    azure_search_key=vector_store_password,\\n    index_name=index_name,\\n    embedding_function=embeddings.embed_query,\\n    additional_search_client_options={\"retry_total\": 4},\\n)\\nretriever_azure = vectorstore_azure.as_retriever()\\n\\nvectorstore_azure.add_documents(documents=docs)\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up Azure Search\n",
    "\"\"\"\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "vector_store_address: str = \"https://ragsearchone.search.windows.net\"\n",
    "vector_store_password: str = \"oZfKSbPgirnz2DlPhtHA8KQjcJ8UaRKnpANa7UI16EAzSeC0NfRj\"\n",
    "\n",
    "index_name: str = \"langchain-rag-prototype\"\n",
    "vectorstore_azure = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    additional_search_client_options={\"retry_total\": 4},\n",
    ")\n",
    "retriever_azure = vectorstore_azure.as_retriever()\n",
    "\n",
    "vectorstore_azure.add_documents(documents=docs)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VECTOR STORE USING CHROMA_DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['560905db-aa54-4524-9c5c-c06ff37c2843',\n",
       " '3568d4b4-04c1-4f79-a981-3557e6191c83',\n",
       " 'ec0785ad-9e5a-45da-bf80-64177197c3b2',\n",
       " '48a02127-a896-4fe8-8a81-667b15acad16',\n",
       " 'a252eea9-9eff-4b41-957f-cf74a2f354cf',\n",
       " 'e0760074-11ed-4774-9973-09a139b3ef99']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorstore_chroma = Chroma.from_documents(splits, embeddings)\n",
    "\n",
    "retriever_chroma = vectorstore_chroma.as_retriever()\n",
    "\n",
    "vectorstore_chroma.add_documents(documents=docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VECTOR STORE USING FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvectorstore_faiss = FAISS.from_documents(docs, embeddings)\\n\\nretriever_faiss = vectorstore_faiss.as_retriever()\\n\\nvectorstore_faiss.add_documents(documents=docs)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "vectorstore_faiss = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "retriever_faiss = vectorstore_faiss.as_retriever()\n",
    "\n",
    "vectorstore_faiss.add_documents(documents=docs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retrieval-Augmented Generation System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrag_chain_faiss = (\\n    {\"context\": retriever_faiss | format_docs, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | llm \\n    | StrOutputParser()\\n)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rag prompt for retrieval\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs): \n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_chroma = (\n",
    "    {\"context\": retriever_chroma | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "rag_chain_azure = (\n",
    "    {\"context\": retriever_azure | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "rag_chain_faiss = (\n",
    "    {\"context\": retriever_faiss | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the total purchase price of ZeniMax Media Inc.?\n",
      "Chroma:\n",
      "Answer: The total purchase price of ZeniMax Media Inc. was $8.1 billion, consisting primarily of cash. The purchase price included $766 million of cash and cash equivalents acquired.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  When did Microsoft complete the acquisition of ZeniMax Media Inc.?\n",
      "Chroma:\n",
      "Answer: Microsoft completed the acquisition of ZeniMax Media Inc. on March 9, 2021. The purchase price was $8.1 billion, primarily consisting of cash. ZeniMax is one of the largest privately held game developers and publishers in the world.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  What is the name of the parent company of Bethesda Softworks LLC?\n",
      "Chroma:\n",
      "Answer: The name of the parent company of Bethesda Softworks LLC is ZeniMax Media Inc.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  How much cash and cash equivalents were acquired as part of the ZeniMax Media Inc. acquisition?\n",
      "Chroma:\n",
      "Answer: The purchase price of ZeniMax Media Inc. included $766 million of cash and cash equivalents acquired.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  To which segment of Microsoft is ZeniMax Media Inc. reported?\n",
      "Chroma:\n",
      "Answer: ZeniMax Media Inc. is reported as part of Microsoft's More Personal Computing segment. The financial results of ZeniMax have been included in Microsoft's consolidated financial statements since the date of the acquisition.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  When was the allocation of the purchase price to goodwill for the ZeniMax Media Inc. acquisition completed?\n",
      "Chroma:\n",
      "Answer: The allocation of the purchase price to goodwill for the ZeniMax Media Inc. acquisition was completed on December 31, 2021.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  How much goodwill was assigned to Microsoft's More Personal Computing segment due to the ZeniMax Media Inc. acquisition?\n",
      "Chroma:\n",
      "Answer: The goodwill assigned to Microsoft's More Personal Computing segment due to the ZeniMax Media Inc. acquisition was $ 5,510 million.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  What is the expected average life of the technology-based intangible assets acquired from ZeniMax Media Inc.?\n",
      "Chroma:\n",
      "Answer: The expected average life of the technology-based intangible assets acquired from ZeniMax Media Inc. is 4 years.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  What is the purchase price per share offered by Microsoft for Activision Blizzard\n",
      "Chroma:\n",
      "Answer: Microsoft offered to purchase Activision Blizzard for $95.00 per share in an all-cash transaction. The total value of the transaction is $68.7 billion, including Activision Blizzard's net cash.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  Inc.?\n",
      "Chroma:\n",
      "Answer: I do not know the answer to this question. The provided context does not mention anything about Inc.?\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  When did Microsoft enter into a definitive agreement to acquire Activision Blizzard\n",
      "Chroma:\n",
      "Answer: Microsoft entered into a definitive agreement to acquire Activision Blizzard on January 18, 2022.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  Inc.?\n",
      "Chroma:\n",
      "Answer: The provided context does not mention anything about Inc. so I cannot answer this question from the provided context.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  What is the total enterprise value of Microsoft's acquisition of Activision Blizzard\n",
      "Chroma:\n",
      "Answer: The provided context does not include information about the total enterprise value of Microsoft's acquisition of Activision Blizzard. Therefore, I cannot answer the question.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  Inc.?\n",
      "Chroma:\n",
      "Answer: The provided context does not mention Inc., so I cannot answer this question.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  What is the name of the segment in Microsoft with the highest goodwill as of June 30 2023?\n",
      "Chroma:\n",
      "Answer: The provided text doesn't explicitly mention the segment with the highest goodwill as of June 30, 2023, so I cannot answer this question from the provided context.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Question:  How much did the goodwill of the Productivity and Business Processes segment increase from June 30 2021 to June 30 2022?\n",
      "Chroma:\n",
      "Answer: The provided text does not mention the Goodwill of the Productivity and Business Processes segment, so I cannot answer this question.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#questions = \"When does Apple recognize revenue for its products?, How does Apple allocate revenue for arrangements with multiple performance obligations?, What are the three performance obligations identified by Apple for iPhone Mac and iPad sales?, How is revenue allocated to product-related bundled services and unspecified software upgrade rights recognized?, What factors does Apple consider when determining control of third-party products?, How does Apple account for third-party application-related sales through the App Store?\"\n",
    "#questions = \"How much deferred revenue was recognized by Apple in the year 2023?, How much deferred revenue from the previous year was recognized by Apple in the year 2022?, What amount from the year prior to that was recognized as part of Apple's net sales for the year ending on September 26th?, In which region did iPhone sales represent a higher proportion than usual according to Note 13?, As of September 30th what is the total amount reported for total deferred revenues?,\" \n",
    "#questions = \"What is Apple's reported amount for total deferred revenues as of December 24th?\"\n",
    "#questions = \"How many shares did Uber issue and sell during its IPO?, What was the price per share during Uberâ€™s IPO?, How much net proceeds did Uber receive from the IPO?, How many shares of common stock were converted from the outstanding redeemable convertible preferred stock upon the IPO?, What was the tax withholding obligation based on the IPO public offering price?, How much did Uber agree to pay for its controlling interest in Cornershop?\"\n",
    "\n",
    "#questions = \"How many shares of common stock were issued upon conversion of the 2021 and 2022 Convertible Notes?, How many shares were issued related to the vesting of RSUs with performance-based vesting conditions?, What was the tax withholding obligation based on the IPO public offering price?, How much stock-based compensation expense did Uber recognize upon its IPO?, How much deferred revenue was recognized by the company in the year 2023?\"\n",
    "#questions = \"What did holders of the 2021 and 2022 Convertible Notes do after the closing of the IPO??\"\n",
    "#questions = \"Who are ZeniMax Media?, Who are activision blizzard inc and what happened between them and microsoft?\"\n",
    "questions = \"What was the total purchase price of ZeniMax Media Inc.?, When did Microsoft complete the acquisition of ZeniMax Media Inc.?, What is the name of the parent company of Bethesda Softworks LLC?, How much cash and cash equivalents were acquired as part of the ZeniMax Media Inc. acquisition?, To which segment of Microsoft is ZeniMax Media Inc. reported?, When was the allocation of the purchase price to goodwill for the ZeniMax Media Inc. acquisition completed?, How much goodwill was assigned to Microsoft's More Personal Computing segment due to the ZeniMax Media Inc. acquisition?, What is the expected average life of the technology-based intangible assets acquired from ZeniMax Media Inc.?, What is the purchase price per share offered by Microsoft for Activision Blizzard, Inc.?, When did Microsoft enter into a definitive agreement to acquire Activision Blizzard, Inc.?, What is the total enterprise value of Microsoft's acquisition of Activision Blizzard Inc.?, What is the name of the segment in Microsoft with the highest goodwill as of June 30 2023?, How much did the goodwill of the Productivity and Business Processes segment increase from June 30 2021 to June 30 2022?\"\n",
    "\n",
    "list = questions.split(\",\")\n",
    "\n",
    "for num, question in enumerate(list):\n",
    "    print(\"Question: {}\".format(question))\n",
    "    response_chroma = rag_chain_chroma.invoke(question)\n",
    "    #response_faiss = rag_chain_faiss.invoke(question)\n",
    "    #response_azure = rag_chain_azure.invoke(question)\n",
    "    #answer = response[\"answer\"]\n",
    "    #print(\"Question: {} \\n Answer: {}\".format(question, answer))\n",
    "    #print(\"AZURE:\")\n",
    "    #print(\"Answer: {}\".format(response_azure))\n",
    "    #print(\"\")\n",
    "    print(\"Chroma:\")\n",
    "    print(\"Answer: {}\".format(response_chroma))\n",
    "    #print(\"\")\n",
    "    #print(\"faiss:\")\n",
    "    #print(\"Answer: {}\".format(response_faiss))\n",
    "    print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total enterprise value of Microsoft's acquisition of Activision Blizzard Inc. is $68.7 billion. This includes Activision Blizzard's net cash.\n"
     ]
    }
   ],
   "source": [
    "# Retrieval using Gemini\n",
    "\"\"\"\n",
    "response = rag_chain.invoke({\"input\": \"During Uber's IPO, how many shares of common stock were sold, at what price were they sold, and how much were the net proceeds received?\"})\n",
    "response[\"answer\"]\n",
    "\"\"\"\n",
    "\n",
    "response = rag_chain_chroma.invoke(\"What is the total enterprise value of Microsoft's acquisition of Activision Blizzard Inc.?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In progress: Adding chat history as context to the rag chain so that the chatbot has even more context and chat seems natural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
